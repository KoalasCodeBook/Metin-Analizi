{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToStr(MyList):\n",
    "    text = \"\"\n",
    "    for i in MyList:\n",
    "        text += i + \" , \"\n",
    "\n",
    "    return text\n",
    "\n",
    "def listToStrBackwards(myString):\n",
    "    list = myString.split(\" , \")[:-1]\n",
    "    return(list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQL Database\n",
    "#articles1 test\n",
    "#articles2 all selcuk \n",
    "import sqlite3 as sql\n",
    "\n",
    "\n",
    "SQLPATH = \"scienedirect.db\"\n",
    "\n",
    "con = sql.connect(SQLPATH)\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS Articles2(\n",
    "        status TEXT,\n",
    "        title TEXT,\n",
    "        link TEXT,\n",
    "        subtype TEXT,\n",
    "        author TEXT\n",
    "    )\"\"\") \n",
    "cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS ArtikelContent(\n",
    "        title TEXT,\n",
    "        abstract TEXT,\n",
    "        abstract2 TEXT,\n",
    "        indroduction TEXT,\n",
    "        snippets TEXT,\n",
    "        longText TEXT,\n",
    "        author TEXT,\n",
    "        authorAffiliation TEXT,\n",
    "        requestDate TEXT,\n",
    "        html TEXT\n",
    "    )\"\"\")\n",
    "con.commit() \n",
    "\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas\n",
    "\n",
    "\n",
    "con = sqlite3.connect(\"scienedirect.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM ArtikelContent\") \n",
    "rows = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowss = pandas.DataFrame(rows)\n",
    "mytitle = \"An investigation on failed or damaged reinforced concrete structures under their own-weight in Turkey\"\n",
    "rowss [rowss.iloc[:,1] == mytitle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!- -!!!\n",
    "Selçuk Universitesine ait Sciencedirect Makale isim ve Link \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "import sqlite3,time\n",
    "\n",
    "con = sqlite3.connect(\"scienedirect.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "driver = webdriver.Firefox(\"\") \n",
    "driver.page_source\n",
    "\n",
    "\n",
    "SHOW = 100 #tek seferde en fazla 100 Makale gösterilebilir\n",
    "OFFSET = 0 #\n",
    "PAGE = 0 #Başlangıç Sayfası\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "driver.get(f\"https://www.sciencedirect.com/search?affiliations=selcuk&show={str(SHOW)}&offset={str(OFFSET)}\")\n",
    "_ = input(\"Yüklendi mi?\")\n",
    "\n",
    "while True:    \n",
    "    LINK = f\"https://www.sciencedirect.com/search?affiliations=selcuk&show={str(SHOW)}&offset={str(OFFSET)}\" \n",
    "    driver.get(LINK)\n",
    "    time.sleep(5)\n",
    "\n",
    "    html_icerigi = driver.page_source\n",
    "    soup = BeautifulSoup(html_icerigi, 'html.parser')\n",
    "    articles = soup.find(class_='search-result-wrapper')  \n",
    "\n",
    "    PAGE += 1\n",
    "    OFFSET += SHOW\n",
    "\n",
    "    if (articles == None):\n",
    "        print(\"Es gibt das Gesuchte Artikel nicht!!!!\")\n",
    "        break\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            STATUS = article.find(class_='OpenAccessArchive text-xs hor').text\n",
    "            print(STATUS)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            TITLE = article.find(\"a\").text\n",
    "            print(TITLE)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "        try:\n",
    "            LINK = article.find(\"a\")[\"href\"]\n",
    "            print(LINK)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        try:\n",
    "            TRASH_BUT_INFO = soup.select('[class*=\"SubType hor\"]')[0].text\n",
    "            print(TRASH_BUT_INFO)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        try:\n",
    "            AUTHORS = article.select('[class*=\"Authors hor\"]')[0].text\n",
    "            print(AUTHORS)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        try:\n",
    "            cursor.execute(\"INSERT INTO articles2 VALUES (?,?,?,?,?)\", (STATUS, TITLE, LINK,TRASH_BUT_INFO,AUTHORS))\n",
    "            con.commit()\n",
    "        except:\n",
    "            print('SQL FEHLER !!!')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 , time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "con = sqlite3.connect(\"scienedirect.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM articles2\") \n",
    "rows = cursor.fetchall()\n",
    "\n",
    "driver = webdriver.Firefox(\"\")\n",
    "time.sleep(3)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for row in rows[2060:]:\n",
    "    try:    \n",
    "        status = row[0]\n",
    "        title = row[1]\n",
    "        link = row[2]\n",
    "        subtype = row[3]\n",
    "        author = row[4]\n",
    "\n",
    "        count=count+1\n",
    "\n",
    "        driver.get(f\"https://www.sciencedirect.com\"+link)\n",
    "        time.sleep(1)\n",
    "\n",
    "        html_icerigi = driver.page_source\n",
    "        soup = BeautifulSoup(html_icerigi, 'html.parser')    \n",
    "    \n",
    "        print(\"*********\\nYeni Sayfa Yükleniyor\\nCount: \"+ str(count))\n",
    "\n",
    "        #Navigation Bölümün Çıkartılması\n",
    "        targets = soup.find_all(role=\"navigation\")\n",
    "        if targets:\n",
    "            for target in targets:\n",
    "                target.decompose()    \n",
    "\n",
    "            targets = soup.find(role=\"navigation\")\n",
    "            if targets == None:\n",
    "                print(\"Silme İşlemi Başarılıydı. Navigation tamamen silindi.\")\n",
    "        else:\n",
    "            print(\"Navigation hiç bulunamadı. Bu yüzden de silinmesine gerek yok\")\n",
    "\n",
    "        #Article Bölümü var mı? \n",
    "        Article = soup.find_all(class_=\"Article\")\n",
    "        if Article == []:        \n",
    "            print(\"Article bulunamadı\")\n",
    "            print(\"Bu Artikle Atlanmalı\")\n",
    "\n",
    "            #Artikel Bölümü yoksa ve error-card varsa client yeniden başlatılmalı\n",
    "            if soup.find(class_=\"error-card\"):\n",
    "                raise ValueError(\"Client yeniden Başlatılmalı\")\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            print(\"Article Bulundu\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #Yazarlar hakkında bilgi toplamak\n",
    "        button = driver.find_elements(By.ID, \"show-more-btn\") \n",
    "        button[0].click()\n",
    "\n",
    "        html_icerigi = driver.page_source\n",
    "        soup = BeautifulSoup(html_icerigi, 'html.parser')    \n",
    "\n",
    "        zAUTHORaffiliation = ''\n",
    "        for i in soup.find_all(class_=\"affiliation\"):\n",
    "            r1 = i.find('dt').getText() if i.find('dt') else ' ' \n",
    "            r2 = i.find('dd').getText() if i.find('dd') else ' '\n",
    "            zAUTHORaffiliation += r1+ ' -> ' +  r2 + '\\n'            \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        article = soup.find(class_=\"Article\")\n",
    "\n",
    "        zABSTRACT = article.find(id=\"preview-section-abstract\") \n",
    "        zINSTRODUCTION = article.find(id=\"preview-section-introduction\")\n",
    "        zSNIPPETS = article.find(id=\"preview-section-snippets\")\n",
    "\n",
    "        zTITLE = article.find(id=\"screen-reader-main-title\")    \n",
    "        zABSTRACT2 = article.find(id=\"abstracts\")\n",
    "        zLONGText = article.find(id=\"body\")\n",
    "        \n",
    "        zAUTHOR = article.find(class_=\"AuthorGroups\").find_all(\"button\") if (article and article.find(class_=\"AuthorGroups\")) else None\n",
    "        zAUTHOR = listToStr([author.text for author in zAUTHOR]) if zAUTHOR else None\n",
    "\n",
    "        zABSTRACT = zABSTRACT.text if zABSTRACT else \"\"    \n",
    "        zINSTRODUCTION = zINSTRODUCTION.text if zINSTRODUCTION else \"\"\n",
    "        zSNIPPETS = zSNIPPETS.text if zSNIPPETS else \"\"\n",
    "\n",
    "        zTITLE = zTITLE.text if zTITLE else \"\"\n",
    "        zABSTRACT2 = zABSTRACT2.text if zABSTRACT2 else \"\"\n",
    "        zLONGText = zLONGText.text if zLONGText else \"\"\n",
    "        zAUTHOR = zAUTHOR if zAUTHOR else \"\"\n",
    "\n",
    "        cursor.execute(\"INSERT INTO ArtikelContent VALUES (?,?,?,?,?,?,?,?,?,?)\", (zTITLE, zABSTRACT, zABSTRACT2,zINSTRODUCTION,zSNIPPETS,zLONGText,zAUTHOR,zAUTHORaffiliation,time.time(),str(soup)))\n",
    "        con.commit()\n",
    "\n",
    "        #print(\"zABSTRACT*****\\n\", zABSTRACT,\"\\n\",\"zINSTRODUCTION*****\\n\",zINSTRODUCTION,\"\\n\",\"zSNIPPETS*****\\n\",zSNIPPETS,\"\\n\",\"zTITLE*****\\n\",zTITLE,\"\\n\",\"zABSTRACT2*****\\n\",zABSTRACT2,\"\\n\",\"zLONGText*****\\n\",zLONGText,\"\\n\",\"zAUTHOR*****\\n\",zAUTHOR,\"\\n\",\n",
    "\n",
    "    \n",
    "    except ValueError as v:\n",
    "        print(v)\n",
    "        driver.close();     \n",
    "        time.sleep(1)\n",
    "\n",
    "        driver = webdriver.Firefox(\"\")\n",
    "        \n",
    "    except:\n",
    "        print(\"***********************************\\n\")\n",
    "        print(\"Hatalı Count: \",count) \n",
    "        print(\"***********************************\\n\")  \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas \n",
    "\n",
    "con = sql.connect(\"scienedirect.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM ArtikelContent\") \n",
    "rows = cursor.fetchall()\n",
    "\n",
    "koalas = pandas.DataFrame(rows)\n",
    "title = koalas.iloc[:,0]\n",
    "abstract = koalas.iloc[:,1]\n",
    "abstract2 = koalas.iloc[:,2]\n",
    "introduction = koalas.iloc[:,3]\n",
    "snippets = koalas.iloc[:,4]\n",
    "longText = koalas.iloc[:,5]\n",
    "author = koalas.iloc[:,6]\n",
    "authorAffiliation = koalas.iloc[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a -> Selcuk University, Faculty of Pharmacy, D...\n",
       "1       a -> Department of Urology, Selcuk University ...\n",
       "2       a -> Faculty of Nursing, Selcuk University, Ko...\n",
       "3       a -> Computer Engineering, Selcuk University, ...\n",
       "4       a -> Selcuk University, Department of Biochemi...\n",
       "                              ...                        \n",
       "4609      -> Graduate School of Pharmaceutical Science...\n",
       "4610      -> Department of Food Science and Technology...\n",
       "4611                                                     \n",
       "4612    a -> Institutes for Systems Genetics, Frontier...\n",
       "4613      -> Physiotherapy, Health Rehabilitation Scie...\n",
       "Name: 7, Length: 4614, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetLetter=[]\n",
    "trash=[]\n",
    "\n",
    "t = authorAffiliation[1773].split(\"\\n\")\n",
    "if ('' in t) : t.remove('')   \n",
    "if len(t) == 1:\n",
    "    print('Hepsi Selçuktandır -> 999')\n",
    "    targetLetter.append('999')\n",
    "else:\n",
    "    for i in t:\n",
    "        i = i.lower()\n",
    "        if 'selcuk' in i:targetLetter.append(i[0])\n",
    "        elif 'selçuk' in i:targetLetter.append(i[0])\n",
    "        else: trash.append(i[0]) \n",
    "\n",
    "print('target: ' + ','.join(targetLetter))\n",
    "print('trash: ' + ','.join(trash))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerekli Func\n",
    "def readPDF(path:str):\n",
    "    from pypdf import PdfReader\n",
    "    PATH_TO_PDF = path\n",
    "    PDF_TEXT = ''\n",
    "\n",
    "    reader = PdfReader(PATH_TO_PDF)\n",
    "\n",
    "    for page in reader.pages:\n",
    "        PDF_TEXT += page.extract_text()\n",
    "        \n",
    "    return(PDF_TEXT)\n",
    "\n",
    "def combineTexts(*args):\n",
    "    finalText = ' '.join(map(str,args))\n",
    "    return finalText\n",
    "\n",
    "def kelimeBulutu(text):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "    wc = WordCloud(\n",
    "            background_color = 'black',\n",
    "            contour_width = 2,\n",
    "            contour_color = 'black',\n",
    "            colormap = 'BuPu_r',\n",
    "            width = 800,\n",
    "            height = 500\n",
    "        ).generate(text)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wc)\n",
    "\n",
    "def kelimeSayacı(text):\n",
    "    from collections import Counter\n",
    " \n",
    "    kelimeler = text.lower().split()\n",
    "    frekans = Counter(kelimeler)\n",
    "    print(frekans)\n",
    "\n",
    "def anahtarKelimeler(text):\n",
    "    #2. Özel Kodlama: Anahtar Kelime Çıkartma \n",
    "    #Anahtar kelime çıkartma, metin analizinde önemli bir adımdır.\n",
    "    #Kütüphane Önerisi: RAKE (Rapid Automatic Keyword Extraction), KeyBERT\n",
    "\n",
    "    #pip install keybert\n",
    "    from keybert import KeyBERT\n",
    "\n",
    "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    anahtar_kelime = model.extract_keywords(text, keyphrase_ngram_range=(1, 2),stop_words='english', top_n=3)\n",
    "    print(anahtar_kelime)\n",
    "\n",
    "def duyguAnalizi(text):\n",
    "    from textblob import TextBlob\n",
    "    from textblob import Word\n",
    "     \n",
    "    # Metni TextBlob ile analiz et     \n",
    "    blob = TextBlob(text)\n",
    "    print(f\"Duygu Puanı: {blob.sentiment.polarity}\")  # -1 ile 1 arasında bir değer\n",
    "    print(f\"Duygu Yoğunluğu: {blob.sentiment.subjectivity}\")  # 0 ile 1 arasında bir değer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Encapsulation of diuron with Zn-based magnetic metal-organic framework and reduction of its mobility in soil'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas.iloc[4,:][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\furka\\Desktop\\Metin Analizi 2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data science', 0.8135), ('deep learning', 0.6818), ('science', 0.658)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "text = \"Deep Learning Approaches to Data Science\"\n",
    "anahtarKelimeler(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wound healing', 0.6019), ('nanofibers evaluation', 0.4655), ('functionalized calix', 0.3962)]\n"
     ]
    }
   ],
   "source": [
    "anahtarKelimeler(title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('biochemical data', 0.3809), ('biochemical analysis', 0.3582), ('extract pharmaceuticals', 0.3521)]\n"
     ]
    }
   ],
   "source": [
    "anahtarKelimeler(abstract[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746742"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = koalas.iloc[:,1]\n",
    "titleTEXT = \"\"\n",
    "\n",
    "for textGroup in titles:\n",
    "    for text in textGroup.split(\" \"):\n",
    "        titleTEXT += \" \" + text\n",
    "        \n",
    "len(titleTEXT.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = koalas.iloc[:,1]\n",
    "titleTEXT = \"\"\n",
    "\n",
    "for textGroup in titles:\n",
    "    for text in textGroup.split(\" \"):\n",
    "        titleTEXT += \" \" + text\n",
    "        \n",
    "len(titleTEXT.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nKelime Sayacı:')\n",
    "kelimeSayacı(titleTEXT)\n",
    "\n",
    "print('\\nDuygu Analizi:')\n",
    "duyguAnalizi(titleTEXT)\n",
    "\n",
    "kelimeBulutu(titleTEXT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
